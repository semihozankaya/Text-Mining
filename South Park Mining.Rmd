---
title: "Text Mining - South Park"
author: "Ozan Kaya"
date: "May 9, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment= FALSE, warning=FALSE, message = FALSE, results = "hide")
library(tidyverse)
library(tidytext)
library(stringr)
library(DT)
library(igraph)
library(ggraph)
library(topicmodels)
library(tm)
library(wordcloud)
library(h2o)
library(textdata)
h2o.init()
```

## Introduction

This simple paper will try to explore basic patterns within the scripts of the famous tv show, South Park. More information on the show can be found [here](https://www.imdb.com/title/tt0121955/?ref_=nv_sr_srsg_0). The scripts include all the spoken text in the show, starting from the first episode and ranges to the last episode of the 18th season. We are able to conduct analysis thanks to Bob Adams courtesy and can be found in his github repository, [here](https://raw.githubusercontent.com/BobAdamsEE/SouthParkData/master/All-seasons.csv). 

Unlike the familiar form of numerical or categorical data, we now have in our hands structured text which we will use to explore and see main properties of the scrip we have. Later on, we will conduct Latent Dirichlet allocation for unsupervised classification of our document into distinct topics tu better understand what we have. Finally, we will train a random forest to build a predictive model to understand if a given line belongs to a certain character. 
```{r}
df <- read_csv("https://raw.githubusercontent.com/semihozankaya/Text-Mining/master/Data/All-seasons.csv")
```

## Exploratory Analysis

We will start with figuring out the 10 most active characters in terms of the number of lines associated to them along with the 20 most spoken words on the series. The results can be seen at the below bar graphs where Eric Cartman is by far the most actice character in our group, followed by Stan and Kyle. Butters and Randy, not surprisingly, comes at 4th and 5th place with very similar amount of spoken lines. Kenny, the silent character that he is, follows the rest with only 881 lines and stands at 8th most active character in the series. 

Similarly, after removing certain stop words from the list, we can also see the most spoken words on the series along with a wordcloud presented below. People comes first, mostly as a way of addressing groups of people but also for distinguishing crab and sea people that takes place in the series. Most of the active character's names appear on the list along with god, hell and time (sounds like an existential masterpiece already). 


```{r}

my_stop_words <- stop_words$word
my_stop_words <- append(my_stop_words, c("yeah", "gonna", "uh", "hey", "huh"), after = length(my_stop_words))


top_chars <- df %>% group_by(Character) %>% tally(sort = TRUE)

## Top words
top_words <- df %>%
  unnest_tokens(word, Line) %>%
  filter(!word %in% my_stop_words) %>%
  count(word,sort = TRUE) %>%
  ungroup() %>%
  mutate(Word = factor(word, levels = rev(unique(word)))) %>% select(Word, Count = n)
```

```{r, out.width='50%'}
## Most active characters
ggplot(head(top_chars,10), aes(x = reorder(Character, n), y = n)) +
  geom_bar(stat='identity',colour="white", fill = "#e6550d") +
  geom_text(aes(x = Character, y = 1, label = paste0("(",n,")",sep="")),
  hjust=0, vjust=.5, size = 4, colour = 'black',
  fontface = 'bold') +
  labs(x = 'Character', y = 'Line Count') +
  coord_flip() + 
  theme_bw()

ggplot(head(top_words, 20), aes(x = Word,y = Count)) +
  geom_bar(stat='identity',colour="white", fill = "#e6550d") +
  geom_text(aes(x = Word, y = 1, label = paste0("(",Count,")",sep="")),
  hjust=0, vjust=.5, size = 4, colour = 'black',
  fontface = 'bold') +
  labs(x = 'Word', y = 'Word Count') + 
  coord_flip() + 
  theme_bw()

```

```{r}
top_words %>% head(100) %>%
  with(wordcloud(Word, Count, max.words = 100,colors=brewer.pal(8, "Dark2")))
```

## TF IDF

Even though understanding the most occuring sentences and/or most active characters in a given text is certainly fun, it would be infinitely better if we could do more with the data we have in our hands. Ultimately, we might be interested in quantifying what actually the document is about. One way to do it is to quantify how important a single word is and that can be done with measuring its term frequency (TF). Another approach is the measure the words' inverse document frequency (IDF) which basically works with decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection of documents. 

```{r}
#Get the Top 20 Characters
top10_chars <- head(top_chars, 10)$Character

Words <- df %>%
  unnest_tokens(word, Line) %>%
  count(Character, word, sort = TRUE) %>%
  ungroup()

Words[which(Words$word == "mkay"), 2] <- "m'kay"
Words[which(Words$word == "hm'kay"), 2] <- "m'kay"
Words[which(Words$word == "uhkay"), 2] <- "m'kay"

total_words <- Words %>% 
  group_by(Character) %>% 
  summarize(total = sum(n))
  
Words <- left_join(Words, total_words)

WordsFull <- Words %>%
  filter(!is.na(Character)) %>%
  bind_tf_idf(word, Character, n)

Words_top10 <- Words %>% filter( Character %in% top10_chars) %>%
  bind_tf_idf(word, Character, n)

plot_td_idf <- Words_top10 %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))
```

```{r}
plot_td_idf %>% 
  top_n(15) %>%
  ggplot(aes(word, tf_idf, fill = Character)) +
  geom_col() +
  labs(x = NULL, y = "tf-idf") +
  coord_flip() + scale_fill_brewer(type = "qual") + 
  theme_bw()

```

There seems to be a problem with Kenny's lines. When taking a better look, we can see that some of the words are coming from the episode 'Titties and Dragons' where Kenny plays an anime princess and helps Sony win a war against Microsoft. The sentences doesn't make much sense and all consist of made up words.  They are quite repetitive and make the td-idf measures get misleading results. 

Words like quack, te and io is also misleading. io and te comes from a spanish-like singing he makes at 4th season episode 3. quack is a repetitive word from the sentence quack-quack. This is misleading as well. Let's add them to our stop word lists so that we can have more insightfull results.

```{r}

mydata <- df %>% filter(Character == "Kenny")
kennys_stop_words <- filter(df, Character == 'Kenny')[which(grepl("Kenni", mydata$Line)),] %>% 
  unnest_tokens(word, Line) %>% select(word)

my_stop_words <- append(my_stop_words, kennys_stop_words[[1]], after = length(my_stop_words))
my_stop_words <- append(my_stop_words, c("quack", "io","te"), after = length(my_stop_words))



Words <- df %>%
  unnest_tokens(word, Line) %>% filter(!word %in% my_stop_words) %>%
  count(Character, word, sort = TRUE) %>%
  ungroup()

Words[which(Words$word == "mkay"), 2] <- "m'kay"
Words[which(Words$word == "hm'kay"), 2] <- "m'kay"
Words[which(Words$word == "uhkay"), 2] <- "m'kay"

total_words <- Words %>% 
  group_by(Character) %>% 
  summarize(total = sum(n))

Words <- left_join(Words, total_words)

WordsFull <- Words %>%
  filter(!is.na(Character)) %>%
  bind_tf_idf(word, Character, n)

Words_top10 <- Words %>% filter( Character %in% top10_chars) %>%
  bind_tf_idf(word, Character, n)

plot_td_idf <- Words_top10 %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))
```

```{r}
plot_td_idf %>% 
  top_n(15) %>%
  ggplot(aes(word, tf_idf, fill = Character)) +
  geom_col() +
  labs(x = NULL, y = "tf-idf") +
  coord_flip() + scale_fill_brewer(type = "qual") + 
  theme_bw()
```

Now it looks better. m'kay, woohoo or loo still doesn't make much sense but the characters are well known for these lines so I decided not to adjust them. Same goes for the names, i.e. Sharon, Stanley and Randy. Sharon and Randy seems to communicate with each other by addressing their names first. 

## Bigrams

So far, we have conducted our simple analyses taking a single word as our unit of measure. Even though it is a completely feasible method to take single words into consideration, it is also possible to look at the relationship between the words as well. We will do so by looking at the words that follows each other in our text, and form bigrams. 

Below, you can see two graphs that shows exactly this relationships. The graphs are a replica of each other and only differ in formatting. They show the most occuring word pairs in our scripts. It gives us global warming, jesus christ and also holy shit. Faith hilling, peruvian flute bands, casa bonita and others would be rather familiar to people that have watched to show at least once. 


```{r}
  
bigrams <- df %>%
    unnest_tokens(bigram, Line, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% my_stop_words,
           !word2 %in% my_stop_words) %>%
    count(word1, word2, sort = TRUE)
  
bigrams <- bigrams %>% filter(word1 != word2)


bigram_graph <- bigrams %>%
  filter(n > 25) %>%
  graph_from_data_frame()

set.seed(09052021)
```

```{r, out.width= '50%'}
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

## Sentiment Analysis

So far, we have investigated how often we see a certain word in our document as well as how often does a character speaks. We take a look which words stands out in comparison to others and what are their relationships with other words. We can now delve into sentiment analysis. We can, with the help of a sentiment lexicon, quantify the emotional content of a text. 

I will use AFINN sentiment lexicon for our purposes here. It provides a numeric score that ranges from 5 to -5 for each word which I will weight and visualize at the end. The two bar graphs below are results of such an endeavour. The one on the left shows how "positive" a character is. The one on the right shows the words' contribution to overall sentiment of the text.

It turns out that Jimmy is the most positive character we have on the show by some margin. Liane Cartman follows him with the only other person that has a positive sentiment. This is not actually surprising. Sheila Brofwlowski and the Announcer, aka the news reporter follows them with a rather neutral score. Kenny, Jimbo, Kyle and Mr. Garrison are the ones that have the lowest sentiment scores followed by the other loveable South Park characters.
The words' contribution on the other hand is rather straightforward. Words such as love and wow contributed to the overall sentiment of the show whereas words like hell, ass and bitch drives the overall sentiment down.


```{r}
my_stop_words <- append(my_stop_words, c("no", "yes"), after = length(my_stop_words))


sentiments <- Words %>% filter(!word %in% my_stop_words) %>%
    inner_join(get_sentiments("afinn"), by = "word") %>%
    group_by(Character) %>%
    summarize(score = sum(value * n) / sum(n)) 

top20_chars <- head(top_chars, 20)$Character

sentiments <- sentiments %>% filter(Character %in% top20_chars)
sentiments <- sentiments %>% arrange(-score)

## Sentiments by words

contributions <- df %>%
  unnest_tokens(word, Line) %>% filter(!word %in% my_stop_words) %>%
  count(Character, word, sort = TRUE) %>%
  ungroup() %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  summarize(occurences = n(),
            contribution = sum(value))


```

```{r, out.width= '50%'}
sentiments %>%
    mutate(Name = reorder(Character, score)) %>%
    ggplot(aes(Name, score, fill = score > 0)) +
    geom_col(show.legend = TRUE) +
    coord_flip() +
    ylab("Average sentiment score") + theme_bw()


contributions %>%
  top_n(20, abs(contribution)) %>%
  mutate(word = reorder(word, contribution)) %>%
  ggplot(aes(word, contribution, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() + theme_bw()

```

## Topic and Predictive Modeling

After our exploratory analysis of the text we have and the simple tf-idf measure and the sentiment analysis we have checked, we can now try to implement some (un)supervised learning on our data. To this end, I will start with topic modeling using Latent Dirichlet allocation to see if we can split our text into 2 distinct topic groups. Later on, I will build a model where I will try to predict if a given text belongs to Cartman first and also to Stan later as well. 

I first start with LDA as promised. Topic modeling, similar to clustering will try to find unspervised patterns in our text and I have tried to split our text into two accordingly. After some clearing and adjustments on the data, we can see that two separate groups can be found in our text as can be seen below. The split doesn't seem to offer much insight and more likely to reflect positive and negative sentiments in the given text. The first group seems to be more "cautious" whereas the other offers a more assertive tone but overall, the split is hardly intiutive.

```{r}

mydf <- df %>%
  select(Character,Line)

corpus_lda <- Corpus(VectorSource(mydf$Line))

LowIDF <- WordsFull %>%
  arrange((idf)) %>%
  select(word,idf)

Low_IDF <- unique(LowIDF$word)

corpus_lda <- tm_map(corpus_lda, tolower)
corpus_lda <- tm_map(corpus_lda, removePunctuation)
corpus_lda <- tm_map(corpus_lda, removeWords, stopwords("english"))
corpus_lda <- tm_map(corpus_lda, removeWords, Low_IDF[1:50])
corpus_lda <- tm_map(corpus_lda, stemDocument)


dtm_lda <- DocumentTermMatrix(corpus_lda)

# Remove sparse terms

dtm_lda <- removeSparseTerms(dtm_lda, 0.997)

# Create data frame

labeledTerms_lda <- as.data.frame(as.matrix(dtm_lda))
labeledTerms_lda <- labeledTerms_lda[rowSums(abs(labeledTerms_lda)) != 0,]

spark_lda <- LDA(labeledTerms_lda, k = 2, control = list(seed = 09052021))

spark_topics <- tidy(spark_lda, matrix = "beta")

spark_top_terms <- spark_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
```

```{r}
spark_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() + theme_bw()
```


```{r}
##### predictive modeling

corpus_cartman <- Corpus(VectorSource(df$Line))

# Pre-process data

corpus_cartman <- tm_map(corpus_cartman, tolower)
corpus_cartman <- tm_map(corpus_cartman, removePunctuation)
corpus_cartman <- tm_map(corpus_cartman, removeWords, stopwords("english"))
corpus_cartman <- tm_map(corpus_cartman, stemDocument)
dtm_cartman <- DocumentTermMatrix(corpus_cartman)

# Remove sparse terms

dtm_cartman <- removeSparseTerms(dtm_cartman, 0.997)


# Create data frame

labeledTerms_cartman <- as.data.frame(as.matrix(dtm_cartman))

my_df <- df %>%
  mutate(is_Cartman = 0)

my_df <- my_df %>%
  mutate(is_Cartman=replace(is_Cartman, Character == 'Cartman', 1)) %>%
  as.data.frame()



labeledTerms_cartman$is_Cartman = as.factor(my_df$is_Cartman)

## Preparing the features for the XGBoost Model

features <- colnames(labeledTerms_cartman)



for (f in features) {
  
  if ((class(labeledTerms_cartman[[f]])=="factor") || (class(labeledTerms_cartman[[f]])=="character")) {
    levels <- unique(labeledTerms_cartman[[f]])
    labeledTerms_cartman[[f]] <- as.numeric(factor(labeledTerms_cartman[[f]], levels=levels))
  }
  
}

## Creating our predictive model

labeledTerms_cartman$is_Cartman = as.factor(labeledTerms_cartman$is_Cartman)


h2o_data_cartman <- as.h2o(labeledTerms_cartman)
splitted_data <- h2o.splitFrame(h2o_data_cartman, ratios = c(0.2), seed = 20210905)

data_train_cartman <- splitted_data[[1]]
data_test_cartman <- splitted_data[[2]]

y_cartman <- "is_Cartman"
X_cartman <- setdiff(names(h2o_data_cartman), c(y_cartman))

# For modeling
foresting_params_cartman <- list(
  ntrees = c(10, 25), 
  mtries = c(10, 15), 
  sample_rate = c(0.2),
  max_depth = c(10, 20)
)

forester_grid_cartman <- h2o.grid(
  "randomForest", x = X_cartman, y = y_cartman,
  training_frame = data_train_cartman,
  grid_id = "forester_cartman",
  nfolds = 5,
  seed = 20210905,
  hyper_params = foresting_params_cartman
)
best_forest_cartman <- h2o.getModel(
  h2o.getGrid(forester_grid_cartman@grid_id, "auc")@model_ids[[sum(lengths(h2o.getGrid(forester_grid_cartman@grid_id, "auc")@model_ids))]]
)


## Lets do Stan


corpus <- Corpus(VectorSource(df$Line))

# Pre-process data

corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
dtm <- DocumentTermMatrix(corpus)

# Remove sparse terms

dtm <- removeSparseTerms(dtm, 0.997)


# Create data frame

labeledTerms <- as.data.frame(as.matrix(dtm))

my_df <- df %>%
  mutate(is_Stan = 0)

my_df <- my_df %>%
  mutate(is_Stan =replace(is_Stan, Character == 'Stan', 1)) %>%
  as.data.frame()



labeledTerms$is_Stan = as.factor(my_df$is_Stan)

## Preparing the features for our model

features <- colnames(labeledTerms)



for (f in features) {
  
  if ((class(labeledTerms[[f]])=="factor") || (class(labeledTerms[[f]])=="character")) {
    levels <- unique(labeledTerms[[f]])
    labeledTerms[[f]] <- as.numeric(factor(labeledTerms[[f]], levels=levels))
  }
  
}

## Creating the Model

labeledTerms$is_Stan = as.factor(labeledTerms$is_Stan)



h2o_data <- as.h2o(labeledTerms)
splitted_data <- h2o.splitFrame(h2o_data, ratios = c(0.2), seed = 20210905)

data_train <- splitted_data[[1]]
data_test <- splitted_data[[2]]

y <- "is_Stan"
X <- setdiff(names(h2o_data), c(y))

# For modeling
foresting_params <- list(
  ntrees = c(10, 25), 
  mtries = c(10, 15), 
  sample_rate = c(0.2),
  max_depth = c(10, 20)
)

forester_grid_stan <- h2o.grid(
  "randomForest", x = X, y = y,
  training_frame = splitted_data[[1]],
  grid_id = "forester_stan",
  nfolds = 5,
  seed = 20210905,
  hyper_params = foresting_params
)
best_forest_stan <- h2o.getModel(
  h2o.getGrid(forester_grid_stan@grid_id, "auc")@model_ids[[sum(lengths(h2o.getGrid(forester_grid_stan@grid_id, "auc")@model_ids))]]
)


h2o.auc(h2o.performance(best_forest_stan))
h2o.auc(h2o.performance(best_forest_stan, data_test))


h2o.auc(h2o.performance(best_forest_cartman))
h2o.auc(h2o.performance(best_forest_cartman, data_test_cartman))


```

We now continute with our predictive modeling and try to find words that best describes its speaker. In other words, we will try to predict the speaker by looking at the lines. In order to that, I have trained a medium sized random forest and at the end, achieved 'fair' results. Below, you can see the ROC plots for two different predictions. The one on the left shows our prediction for Cartman where the training AUC measure is `r 
h2o.auc(h2o.performance(best_forest_cartman))` and the test AUC is `r h2o.auc(h2o.performance(best_forest_cartman, data_test_cartman))`. On the other hand, the graph on the right shows the same for our prediction for Stan where its training AUC is `r h2o.auc(h2o.performance(best_forest_stan))` and the test measure is `r h2o.auc(h2o.performance(best_forest_stan, data_test))`. These are not terrible results and shows promise.

Following these ROC plots, we can also see the variable importance plots below them for both of the predictions. This is more interesting, at least for me, since it offers a chance to test our results as mere watchers of the show. We can see that the relative variable importance for Cartman are somewhat familiar. The same is hard to say for Stan but in any case, our model is not the best and even though it succeeds more than it fails, its false positive and negative counts are rather high.

```{r, out.width= '50%'}
plot(h2o.performance(best_forest_cartman, xval = TRUE), type = "roc")

plot(h2o.performance(best_forest_stan, xval = TRUE), type = "roc")

```


```{r, out.width= '50%'}
h2o.varimp_plot(best_forest_cartman)

h2o.varimp_plot(best_forest_stan)


```

## Summary

All in all, we have tried to explore the scripts of South Park from its very first episode to the last episode of the 18th season. It was first and foremost fun to do so but it could also be taught of being rather informative. We first plotted the mostly used words and the most active characters in the show. This will definitely give some insight about the show to people that haven't watched it before. We then continue with creating a tf-idf measure for every word and check the measures for the most active characters of the show. This is also another good measure to understand the role of each character in the show. We also checked the sentiment of the overall lines of a given character as well as the the words that contributed to this overall sentiment. The remaining topic and predictive modeling gave further insight to us and overall, it can be argued that our analysis provides a good understanding of the show itself.